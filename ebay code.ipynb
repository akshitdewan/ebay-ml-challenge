{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.cluster._supervised import contingency_matrix, check_clusterings\n",
    "def pair_confusion_matrix(labels_true, labels_pred):\n",
    "    \"\"\"Pair confusion matrix arising from two clusterings.\n",
    "    The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
    "    between two clusterings by considering all pairs of samples and counting\n",
    "    pairs that are assigned into the same or into different clusters under\n",
    "    the true and predicted clusterings.\n",
    "    Considering a pair of samples that is clustered together a positive pair,\n",
    "    then as in binary classification the count of true negatives is\n",
    "    :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
    "    :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
    "    Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_true : array-like of shape (n_samples,), dtype=integral\n",
    "        Ground truth class labels to be used as a reference.\n",
    "    labels_pred : array-like of shape (n_samples,), dtype=integral\n",
    "        Cluster labels to evaluate.\n",
    "    Returns\n",
    "    -------\n",
    "    C : ndarray of shape (2, 2), dtype=np.int64\n",
    "        The contingency matrix.\n",
    "    See Also\n",
    "    --------\n",
    "    rand_score: Rand Score\n",
    "    adjusted_rand_score: Adjusted Rand Score\n",
    "    adjusted_mutual_info_score: Adjusted Mutual Information\n",
    "    Examples\n",
    "    --------\n",
    "    Perfectly matching labelings have all non-zero entries on the\n",
    "    diagonal regardless of actual label values:\n",
    "      >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "      >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "      array([[8, 0],\n",
    "             [0, 4]]...\n",
    "    Labelings that assign all classes members to the same clusters\n",
    "    are complete but may be not always pure, hence penalized, and\n",
    "    have some off-diagonal non-zero entries:\n",
    "      >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
    "      array([[8, 2],\n",
    "             [0, 2]]...\n",
    "    Note that the matrix is not symmetric.\n",
    "    References\n",
    "    ----------\n",
    "    .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
    "      Classification 1985\n",
    "      https://link.springer.com/article/10.1007%2FBF01908075\n",
    "    \"\"\"\n",
    "    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    n_samples = np.int64(labels_true.shape[0])\n",
    "\n",
    "    # Computation using the contingency data\n",
    "    contingency = contingency_matrix(\n",
    "        labels_true, labels_pred, sparse=True\n",
    "        )#, dtype=np.int64)\n",
    "    n_c = np.ravel(contingency.sum(axis=1))\n",
    "    n_k = np.ravel(contingency.sum(axis=0))\n",
    "    sum_squares = (contingency.data ** 2).sum()\n",
    "    C = np.empty((2, 2), dtype=np.int64)\n",
    "    C[1, 1] = sum_squares - n_samples\n",
    "    C[0, 1] = contingency.dot(n_k).sum() - sum_squares\n",
    "    C[1, 0] = contingency.transpose().dot(n_c).sum() - sum_squares\n",
    "    C[0, 0] = n_samples ** 2 - C[0, 1] - C[1, 0] - sum_squares\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "akshit_df = './mlchallenge_set_2021.tsv'\n",
    "akshit_valid = './mlchallenge_set_validation.tsv'\n",
    "#sam_df = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_2021_edited.txt'\n",
    "#sam_valid = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_validation.tsv'\n",
    "SA_valid=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_validation.tsv',header=None)\n",
    "SA_df=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_2021.tsv',header=None)\n",
    "df=SA_df\n",
    "valid=SA_valid\n",
    "#df = pd.read_table(sam_df)\n",
    "#valid = pd.read_table(sam_valid,sep='\\t')\n",
    "df.columns=['category','primary_image_url','All Links','Tags','index']\n",
    "valid.columns=['ID', 'Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "990\n2480\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of groups with list of IDs in that group\n",
    "dict = {}\n",
    "total=0\n",
    "for index,row in valid.iterrows():\n",
    "    if row['Group'] in dict:\n",
    "        dict[row['Group']].append(row['ID'])\n",
    "    else:\n",
    "        dict[row['Group']] = [row['ID']]\n",
    "#list of groups with multiple IDs\n",
    "matches = []\n",
    "for key,value in dict.items():\n",
    "    if len(value) > 1:\n",
    "        total+=len(value)\n",
    "        matches.append(key)\n",
    "print(len(matches))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   category                                  primary_image_url  \\\n0         2  https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/iYYA...   \n1         2  https://i.ebayimg.com/00/s/MTA1OFgxMTM0/z/KPIA...   \n2         2  https://i.ebayimg.com/00/s/MTIwMFgxNjAw/z/flIA...   \n3         2  http://i.ebayimg.com/00/s/ODAwWDEwNjc=/z/XHcAA...   \n4         2  https://i.ebayimg.com/00/s/MTA2N1gxNjAw/z/scsA...   \n\n                                           All Links  \\\n0  https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/iYYA...   \n1  https://i.ebayimg.com/00/s/MTA1OFgxMTM0/z/KPIA...   \n2  https://i.ebayimg.com/00/s/MTIwMFgxNjAw/z/flIA...   \n3  http://i.ebayimg.com/00/s/ODAwWDEwNjc=/z/XHcAA...   \n4  https://i.ebayimg.com/00/s/MTA2N1gxNjAw/z/scsA...   \n\n                                                Tags  index  \n0  (Brand:Shimano,US Shoe Size (Men's):4.5,Modifi...      0  \n1  (Color:Gray/White,Country/Region of Manufactur...      1  \n2  (Style:Cleats,Color:White Orange,US Shoe Size ...      2  \n3  (Width:Medium (D, M),US Size:9,Brand:VANS,Colo...      3  \n4  (US Shoe Size (Men's):10.5,Material:Enter item...      4  \n    ID    Group\n0  163  2000001\n1  194  2000003\n2  251  2000004\n3  315  2000006\n4  321  2000007\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0     [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/iYY...\n1     [https://i.ebayimg.com/00/s/MTA1OFgxMTM0/z/KPI...\n2     [https://i.ebayimg.com/00/s/MTIwMFgxNjAw/z/flI...\n3     [http://i.ebayimg.com/00/s/ODAwWDEwNjc=/z/XHcA...\n4     [https://i.ebayimg.com/00/s/MTA2N1gxNjAw/z/scs...\n                            ...                        \n95    [https://i.ebayimg.com/00/s/MTU5OVgxNTgx/z/vMA...\n96    [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/GoA...\n97    [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/Hm0...\n98    [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/VhA...\n99    [https://i.ebayimg.com/00/s/MTYwMFgxNjAw/z/n4o...\nName: All Links, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#split links into list (for now only does first 100)\n",
    "i = 0\n",
    "for link in df['All Links'][0:100]:\n",
    "    df['All Links'][i] = link.split(';')\n",
    "    i+=1\n",
    "print(df['All Links'][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def register_attributes(attribute, all_attributes):\n",
    "    attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        all_attributes.add(attribute[i][0])\n",
    "    #print(f'atttribute is: {attribute}')\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return(attribute)\n",
    "\n",
    "def map_attributes(attribute, num_attributes, index_to_attr):\n",
    "    attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    all_attributes_for_row = [None] * num_attributes\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        #print(f'index: {attr_to_index[attribute[i][0]]}')\n",
    "        if len(attribute[i]) > 1:\n",
    "            all_attributes_for_row[attr_to_index[attribute[i][0]]] = attribute[i][1]\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return all_attributes_for_row\n",
    "m = 2000\n",
    "all_attributes = set()\n",
    "all_maps = []\n",
    "for index,row in df[0:m].iterrows():\n",
    "    register_attributes(row['Tags'], all_attributes)\n",
    "\n",
    "all_attributes = list(all_attributes)\n",
    "attr_to_index = {all_attributes[i]: i for i in range(len(all_attributes))}\n",
    "#print(attr_to_index)\n",
    "#print(f'numAttributes: {len(all_attributes)}')\n",
    "\n",
    "for index,row in df[0:m].iterrows():\n",
    "    all_maps.append(map_attributes(row['Tags'], len(all_attributes), attr_to_index))\n",
    "#print(all_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'all_maps' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af7a5e5c4a3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#dataframe with attribute values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_maps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_maps' is not defined"
     ]
    }
   ],
   "source": [
    "#dataframe with attribute values\n",
    "categories = pd.DataFrame(all_maps)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'categories' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3e4e19cdd73c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categories' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x = OneHotEncoder().fit_transform(categories)\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(compute_distances=True,compute_full_tree = True,distance_threshold = 1.5,n_clusters=None).fit(x.todense())\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LABELS\")\n",
    "print(clustering.labels_)\n",
    "print(len(clustering.labels_))\n",
    "print(len(set(clustering.labels_)))\n",
    "print(\"DISTAnCES\")\n",
    "print(clustering.distances_)\n",
    "print(\"num connected components\")\n",
    "print(clustering.n_connected_components_)\n",
    "# make this better\n",
    "# make this work on the entire dataset\n",
    "# fix nonetypes\n",
    "# don't punish missing attributes, but punish conflicts. how do we encode this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for i, label in enumerate(clustering.labels_):\n",
    "    if label not in groups:\n",
    "        groups[label] = []\n",
    "    groups[label].append(i)\n",
    "groups = {label: groups[label] for label in groups if len(groups[label]) > 1}\n",
    "print(groups)\n",
    "for label in groups:\n",
    "    print(f'GROUP: {label}')\n",
    "    for item in groups[label]:\n",
    "        print(df['Tags'][item])\n",
    "    print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    #response = requests.get(url)\n",
    "    #img = Image.open(BytesIO(response.content))\n",
    "    link_labels = [df['Tags'][i] for i in clustering.labels_]\n",
    "    dendrogram(linkage_matrix, labels = link_labels)\n",
    "plot_dendrogram(clustering)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple parsing logic for attributes - works well\n",
    "import re\n",
    "from collections import Counter\n",
    "freq=Counter()\n",
    "attribute=[['']]*len(df)\n",
    "trialrange=500\n",
    "for x in range(trialrange):#range(int(len(df)/10)):#len(df)\n",
    "    attribute[x]=df.iloc[x,3].lower()\n",
    "    attribute[x] = re.sub(r'[()]','', attribute[x])\n",
    "    attribute[x] = re.split(r',', attribute[x])\n",
    "    attribute[x] = [a.split(':') for a in attribute[x]]\n",
    "    freq+=Counter([i[0] for i in attribute[x]])\n",
    "    tempdict={}\n",
    "    for i in attribute[x]:\n",
    "\n",
    "            try:\n",
    "                tempdict[i[0]]=float(i[1])\n",
    "            except:\n",
    "                try:\n",
    "                    tempdict[i[0]]=i[1]\n",
    "                except:\n",
    "                    pass\n",
    "    attribute[x]=tempdict\n",
    "\n",
    "df['seg']=attribute\n",
    "#print(df['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "Images=[]\n",
    "print(trialrange)\n",
    "for i in range(trialrange):\n",
    "    try:\n",
    "        #df['seg'].iloc[i]['brand']\n",
    "        #df['primary_image_url'].iloc[i]\n",
    "        #print(df['seg'].iloc[i]['brand'])\n",
    "        #print(df['primary_image_url'].iloc[i])\n",
    "        Brands.append(df['seg'].iloc[i]['brand'])\n",
    "        Images.append(df['primary_image_url'].iloc[i])\n",
    "    except:\n",
    "        Brands.append('None')\n",
    "        print('not possible at: ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "'''\n",
    "url = df['primary_image_url'][4]\n",
    "response = requests.get(url)\n",
    "#img = Image.open(BytesIO(response.content))\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "img.show()\n",
    "result = Image.new(img.mode, (1000, 550), (64,64,64))\n",
    "result.paste(img, (0, 0))\n",
    "result.show()\n",
    "print(np.asarray(img).shape)\n",
    "print(np.asarray(result).shape)\n",
    "'''\n",
    "image_array = []\n",
    "images = []\n",
    "max_height = 0\n",
    "max_width = 0\n",
    "i = 0\n",
    "for img in df['primary_image_url'][0:n]:\n",
    "    response = requests.get(img)\n",
    "    if i%20 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    if np.asarray(img).shape[1] > max_width:\n",
    "        max_width = np.asarray(img).shape[1]\n",
    "    if np.asarray(img).shape[0] > max_height:\n",
    "        max_height = np.asarray(img).shape[0]\n",
    "    images.append(img)\n",
    "i=0\n",
    "for img in images:\n",
    "    if i%20 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    try:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode, (max_width, max_height), (64,64,64))\n",
    "    except:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode,(max_width,max_height),(64,64,64))\n",
    "    margin.paste(img, (0, 0))\n",
    "    image_array.append(np.asarray(margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import sys\n",
    "#print(sys.version)\n",
    "#%pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(1, (3, 3), activation='relu', input_shape=(max_width, max_height, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "#model.add(layers.Dense(4, activation='relu'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_dict = {}\n",
    "num = 0\n",
    "labels = []\n",
    "for b in Brands:\n",
    "    if b not in brand_dict:\n",
    "        brand_dict[b] = num\n",
    "        num+=1\n",
    "    labels.append(brand_dict[b])\n",
    "\n",
    "m = 500\n",
    "n = round(m*.8)\n",
    "train_images = np.asarray(image_array[0:n])\n",
    "test_images = np.asarray(image_array[n:])\n",
    "train_labels = np.asarray(labels[0:n])\n",
    "test_labels = np.asarray(labels[n:])\n",
    "print(type(test_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=5, \n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}