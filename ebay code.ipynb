{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code copied from this source as recommended by Ebay official since current version of scikit-learn does not have pair confusion matrix\n",
    "https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/cluster/_supervised.py\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster._supervised import contingency_matrix, check_clusterings\n",
    "def pair_confusion_matrix(labels_true, labels_pred):\n",
    "    \"\"\"Pair confusion matrix arising from two clusterings.\n",
    "    The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
    "    between two clusterings by considering all pairs of samples and counting\n",
    "    pairs that are assigned into the same or into different clusters under\n",
    "    the true and predicted clusterings.\n",
    "    Considering a pair of samples that is clustered together a positive pair,\n",
    "    then as in binary classification the count of true negatives is\n",
    "    :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
    "    :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
    "    Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    labels_true : array-like of shape (n_samples,), dtype=integral\n",
    "        Ground truth class labels to be used as a reference.\n",
    "    labels_pred : array-like of shape (n_samples,), dtype=integral\n",
    "        Cluster labels to evaluate.\n",
    "    Returns\n",
    "    -------\n",
    "    C : ndarray of shape (2, 2), dtype=np.int64\n",
    "        The contingency matrix.\n",
    "    See Also\n",
    "    --------\n",
    "    rand_score: Rand Score\n",
    "    adjusted_rand_score: Adjusted Rand Score\n",
    "    adjusted_mutual_info_score: Adjusted Mutual Information\n",
    "    Examples\n",
    "    --------\n",
    "    Perfectly matching labelings have all non-zero entries on the\n",
    "    diagonal regardless of actual label values:\n",
    "      >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
    "      >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
    "      array([[8, 0],\n",
    "             [0, 4]]...\n",
    "    Labelings that assign all classes members to the same clusters\n",
    "    are complete but may be not always pure, hence penalized, and\n",
    "    have some off-diagonal non-zero entries:\n",
    "      >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
    "      array([[8, 2],\n",
    "             [0, 2]]...\n",
    "    Note that the matrix is not symmetric.\n",
    "    References\n",
    "    ----------\n",
    "    .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
    "      Classification 1985\n",
    "      https://link.springer.com/article/10.1007%2FBF01908075\n",
    "    \"\"\"\n",
    "    labels_true, labels_pred = check_clusterings(labels_true, labels_pred)\n",
    "    n_samples = np.int64(labels_true.shape[0])\n",
    "\n",
    "    # Computation using the contingency data\n",
    "    contingency = contingency_matrix(\n",
    "        labels_true, labels_pred, sparse=True\n",
    "        )#, dtype=np.int64)\n",
    "    n_c = np.ravel(contingency.sum(axis=1))\n",
    "    n_k = np.ravel(contingency.sum(axis=0))\n",
    "    sum_squares = (contingency.data ** 2).sum()\n",
    "    C = np.empty((2, 2), dtype=np.int64)\n",
    "    C[1, 1] = sum_squares - n_samples\n",
    "    C[0, 1] = contingency.dot(n_k).sum() - sum_squares\n",
    "    C[1, 0] = contingency.transpose().dot(n_c).sum() - sum_squares\n",
    "    C[0, 0] = n_samples ** 2 - C[0, 1] - C[1, 0] - sum_squares\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "akshit_df = './mlchallenge_set_2021.tsv'\n",
    "akshit_valid = './mlchallenge_set_validation.tsv'\n",
    "#sam_df = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_2021_edited.txt'\n",
    "#sam_valid = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_validation.tsv'\n",
    "SA_valid=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_validation.tsv',header=None)\n",
    "SA_df=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_2021.tsv',header=None)\n",
    "df=SA_df\n",
    "valid=SA_valid\n",
    "#df = pd.read_table(sam_df)\n",
    "#valid = pd.read_table(sam_valid,sep='\\t')\n",
    "df.columns=['category','primary_image_url','All Links','Tags','index']\n",
    "valid.columns=['ID', 'Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "990\n2480\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of groups with list of IDs in that group\n",
    "dict = {}\n",
    "total=0\n",
    "for index,row in valid.iterrows():\n",
    "    if row['Group'] in dict:\n",
    "        dict[row['Group']].append(row['ID'])\n",
    "    else:\n",
    "        dict[row['Group']] = [row['ID']]\n",
    "#list of groups with multiple IDs\n",
    "matches = []\n",
    "for key,value in dict.items():\n",
    "    if len(value) > 1:\n",
    "        total+=len(value)\n",
    "        matches.append(key)\n",
    "print(len(matches))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0     [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/iYY...\n1     [https://i.ebayimg.com/00/s/MTA1OFgxMTM0/z/KPI...\n2     [https://i.ebayimg.com/00/s/MTIwMFgxNjAw/z/flI...\n3     [http://i.ebayimg.com/00/s/ODAwWDEwNjc=/z/XHcA...\n4     [https://i.ebayimg.com/00/s/MTA2N1gxNjAw/z/scs...\n                            ...                        \n95    [https://i.ebayimg.com/00/s/MTU5OVgxNTgx/z/vMA...\n96    [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/GoA...\n97    [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/Hm0...\n98    [https://i.ebayimg.com/00/s/MTYwMFgxMjAw/z/VhA...\n99    [https://i.ebayimg.com/00/s/MTYwMFgxNjAw/z/n4o...\nName: All Links, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#split links into list (for now only does first 100)\n",
    "i = 0\n",
    "for link in df['All Links'][0:100]:\n",
    "    df['All Links'][i] = link.split(';')\n",
    "    i+=1\n",
    "print(df['All Links'][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def register_attributes(attribute, all_attributes):\n",
    "    attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        all_attributes.add(attribute[i][0])\n",
    "    #print(f'atttribute is: {attribute}')\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return(attribute)\n",
    "\n",
    "def map_attributes(attribute, num_attributes, index_to_attr):\n",
    "    attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    all_attributes_for_row = [None] * num_attributes\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        #print(f'index: {attr_to_index[attribute[i][0]]}')\n",
    "        if len(attribute[i]) > 1:\n",
    "            all_attributes_for_row[attr_to_index[attribute[i][0]]] = attribute[i][1]\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return all_attributes_for_row\n",
    "m = 2000\n",
    "all_attributes = set()\n",
    "all_maps = []\n",
    "for index,row in df[0:m].iterrows():\n",
    "    register_attributes(row['Tags'], all_attributes)\n",
    "\n",
    "all_attributes = list(all_attributes)\n",
    "attr_to_index = {all_attributes[i]: i for i in range(len(all_attributes))}\n",
    "#print(attr_to_index)\n",
    "#print(f'numAttributes: {len(all_attributes)}')\n",
    "\n",
    "for index,row in df[0:m].iterrows():\n",
    "    all_maps.append(map_attributes(row['Tags'], len(all_attributes), attr_to_index))\n",
    "#print(all_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with attribute values\n",
    "categories = pd.DataFrame(all_maps)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x = OneHotEncoder().fit_transform(categories)\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(compute_distances=True,compute_full_tree = True,distance_threshold = 1.5,n_clusters=None).fit(x.todense())\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LABELS\")\n",
    "print(clustering.labels_)\n",
    "print(len(clustering.labels_))\n",
    "print(len(set(clustering.labels_)))\n",
    "print(\"DISTAnCES\")\n",
    "print(clustering.distances_)\n",
    "print(\"num connected components\")\n",
    "print(clustering.n_connected_components_)\n",
    "# make this better\n",
    "# make this work on the entire dataset\n",
    "# fix nonetypes\n",
    "# don't punish missing attributes, but punish conflicts. how do we encode this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for i, label in enumerate(clustering.labels_):\n",
    "    if label not in groups:\n",
    "        groups[label] = []\n",
    "    groups[label].append(i)\n",
    "groups = {label: groups[label] for label in groups if len(groups[label]) > 1}\n",
    "print(groups)\n",
    "for label in groups:\n",
    "    print(f'GROUP: {label}')\n",
    "    for item in groups[label]:\n",
    "        print(df['Tags'][item])\n",
    "    print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    #response = requests.get(url)\n",
    "    #img = Image.open(BytesIO(response.content))\n",
    "    link_labels = [df['Tags'][i] for i in clustering.labels_]\n",
    "    dendrogram(linkage_matrix, labels = link_labels)\n",
    "plot_dendrogram(clustering)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple parsing logic for attributes - works well\n",
    "import re\n",
    "from collections import Counter\n",
    "freq=Counter()\n",
    "attribute=[['']]*len(df)\n",
    "trialrange=5000\n",
    "for x in range(trialrange):#range(int(len(df)/10)):#len(df)\n",
    "    attribute[x]=df.iloc[x,3].lower()\n",
    "    attribute[x] = re.sub(r'[()]','', attribute[x])\n",
    "    attribute[x] = re.split(r',', attribute[x])\n",
    "    attribute[x] = [a.split(':') for a in attribute[x]]\n",
    "    freq+=Counter([i[0] for i in attribute[x]])\n",
    "    tempdict={}\n",
    "    for i in attribute[x]:\n",
    "\n",
    "            try:\n",
    "                tempdict[i[0]]=float(i[1])\n",
    "            except:\n",
    "                try:\n",
    "                    tempdict[i[0]]=i[1]\n",
    "                except:\n",
    "                    pass\n",
    "    attribute[x]=tempdict\n",
    "\n",
    "df['seg']=attribute\n",
    "#print(df['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "Brands=[]\n",
    "Images=[]\n",
    "Colors = []\n",
    "color_images = []\n",
    "print(trialrange)\n",
    "check=False\n",
    "for i in range(trialrange):\n",
    "    try:\n",
    "        #df['seg'].iloc[i]['brand']\n",
    "        #df['primary_image_url'].iloc[i]\n",
    "        #print(df['seg'].iloc[i]['brand'])\n",
    "        #print(df['primary_image_url'].iloc[i])\n",
    "        if df['seg'].iloc[i]['brand'] == 'nike' or df['seg'].iloc[i]['brand'] == 'adidas':\n",
    "            Brands.append(df['seg'].iloc[i]['brand'])\n",
    "            Images.append(df['primary_image_url'].iloc[i])\n",
    "        if df['seg'].iloc[i]['color'] == 'black' or df['seg'].iloc[i]['color'] == 'white':\n",
    "            Colors.append(df['seg'].iloc[i]['color'])\n",
    "            color_images.append(df['primary_image_url'].iloc[i])\n",
    "    except:\n",
    "        continue\n",
    "        #Brands.remove[-1]\n",
    "        #print('not possible at: ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "print(len(Brands))\n",
    "print(len(Colors))\n",
    "print(len(color_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "0\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "n = 400\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "'''\n",
    "url = df['primary_image_url'][4]\n",
    "response = requests.get(url)\n",
    "#img = Image.open(BytesIO(response.content))\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "img.show()\n",
    "result = Image.new(img.mode, (1000, 550), (64,64,64))\n",
    "result.paste(img, (0, 0))\n",
    "result.show()\n",
    "print(np.asarray(img).shape)\n",
    "print(np.asarray(result).shape)\n",
    "'''\n",
    "image_array = []\n",
    "images = []\n",
    "max_height = 0\n",
    "max_width = 0\n",
    "i = 0\n",
    "for img in color_images[0:n]:\n",
    "    response = requests.get(img)\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    if np.asarray(img).shape[1] > max_width:\n",
    "        max_width = np.asarray(img).shape[1]\n",
    "    if np.asarray(img).shape[0] > max_height:\n",
    "        max_height = np.asarray(img).shape[0]\n",
    "    images.append(img)\n",
    "i=0\n",
    "for img in images:\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    try:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode, (max_width, max_height), (64,64,64))\n",
    "    except:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode,(max_width,max_height),(64,64,64))\n",
    "    margin.paste(img, (0, 0))\n",
    "    image_array.append(np.asarray(margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "        \n",
      " \n",
      "\n",
      "Reread\n",
      "\n",
      "eee FS\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "a, y\n",
      "\n",
      "ahs\n",
      "SS\n",
      "\f\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "FPP TY\n",
      "PPPr eT\n",
      "\n",
      "Pestd © SS.\n",
      "a\n",
      "<\n",
      "ae\n",
      "\n",
      " \n",
      "\f\n",
      "T\n",
      "\n",
      "UBULAR DOOM\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "adidas\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      "= SS\n",
      "{ oO asics. |\n",
      "\n",
      "z [3]\n",
      "NUN\n",
      "\n",
      " \n",
      "\f\n",
      "HM!\n",
      "\n",
      " \n",
      "\n",
      "\"7 te\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "  \n",
      "\n",
      "perenne arena ara ara\n",
      "\n",
      "— deeee q\n",
      "\n",
      "   \n",
      "   \n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "uae?\n",
      "\n",
      "Ceoco\n",
      "\n",
      " \n",
      "\f\n",
      "PT hho\n",
      "eT Alal\n",
      "ee T TTT ad\n",
      "\n",
      "PPT BL Lilell\n",
      "Te\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      "Men's Air Jordan Retro 13\n",
      "Basketball Shoes\n",
      "\n",
      "Elemental Gold/Broque Brown/\n",
      "Gum\n",
      "10.5\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      "US\n",
      "\n",
      "“38: \"5:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "B37572\n",
      "\n",
      "SEFRYE/RAWSTE/RED\n",
      "JASEGI/ACIBRU/ROUGE\n",
      "ORIGINALS\n",
      "\n",
      "ORIGINALS\n",
      "\n",
      "MADE IN CHINA\n",
      "FABRIQUE EN CHINE ABSBOXUS\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "I\n",
      "LEN)\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "&i\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      "JIN;\n",
      "INS,\n",
      "N\n",
      "\n",
      "Y\n",
      "\n",
      "I\n",
      "\n",
      "\\\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "\f\n",
      "\f\n",
      " \n",
      "\f\n",
      "\f\n",
      "=\n",
      "=m\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      "tive\n",
      "\n",
      "rc 9\n",
      "N\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "FANCIHAWAY\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "Roane\n",
      "air PERFOR\n",
      "\n",
      "La\n",
      "\n",
      "peer\n",
      "Ss Sie\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "   \n",
      " \n",
      "\n",
      "Le.\n",
      "\n",
      "rr\n",
      "Ped\n",
      "reo\n",
      "coe)\n",
      "\n",
      "7s\n",
      "\n",
      " \n",
      "\n",
      "ead\n",
      "\n",
      "cS\n",
      "\n",
      "  \n",
      "\n",
      "H\n",
      "i\n",
      "i\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "peste) Pg\n",
      "\n",
      "14\n",
      "\n",
      "   \n",
      "\n",
      "‘Suggested etal: USD $175.00\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "ATAT 12:13 PM 6 62m\n",
      "\n",
      " \n",
      "\n",
      "€ inbox Av\n",
      "\n",
      "OnDER Dare:\n",
      "oanaz017\n",
      "\n",
      " \n",
      "\n",
      "Nike SB Ait Force 2 Low x Supreme\n",
      "Men's Skateboarding Shoe\n",
      "\n",
      "Styler\n",
      "Sizei85\n",
      "ay:\n",
      "\n",
      " \n",
      "\n",
      "YOU MAY ALSO LIKE\n",
      "\n",
      " \n",
      "\n",
      "Q\n",
      "\n",
      "a\n",
      "\f\n",
      "  \n",
      "\n",
      "Ce\n",
      "\n",
      " \n",
      "\n",
      "MAJOR LEAGUE SN rien fe\n",
      "BS\n",
      "\n",
      "OT a er]\n",
      "ase irl\n",
      "\n",
      "a\n",
      "\f\n",
      "   \n",
      "  \n",
      "  \n",
      " \n",
      "\n",
      "Vw\n",
      "Coa\n",
      "@®\n",
      "sommememmmemia |) 912 1/3) 12:| (7 aan “Zi\n",
      "PAG hy: eee 98\n",
      "\n",
      "LUBBOCK. &—~\n",
      "eR Feel\n",
      "\n",
      "ray\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      "F\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "RJ Shoebox uLe\n",
      "mail.com\n",
      "\n",
      "prshoeboxe9)\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "  \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      "Pitt: Pape tees t\n",
      "f ae ap AE ae\n",
      "ge\") oe a\n",
      "ie ae an lye ‘\n",
      "\n",
      "y\n",
      "\n",
      ".\n",
      "\n",
      "  \n",
      "\n",
      "7\n",
      "\n",
      "wt\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\f\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "Thanks, Raymond!\n",
      "We're On It.\n",
      "\n",
      "Your order's in. We're working to get it\n",
      "packed up and out the door—expect a\n",
      "\n",
      "shipping confirmation email soon.\n",
      "\n",
      "Arrives 03/30/2018\n",
      "\n",
      " \n",
      "\n",
      "“Ak beerchines 9 tote Shanta:\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "7.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      "[ FERGZ KICKZ\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "    \n",
      "\n",
      "new balance\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "eee\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "Vintage or New\n",
      "\n",
      "‘ak Be eee ee dai\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "AN =\n",
      "\n",
      "We fi\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "     \n",
      "\n",
      "Pec a\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "LOUIS VUITTON\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "im\n",
      "2\n",
      "m\n",
      "(-]\n",
      "eI\n",
      "FI\n",
      ")\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "         \n",
      "\n",
      " \n",
      "\n",
      "<3 Ra bas Pre\n",
      "ae a\n",
      "\n",
      "qa\\aC\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "   \n",
      "\n",
      "RJ Shoebox LLC\n",
      "RISh @gmail.co\n",
      "\n",
      "\"tan i A\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "  \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\n",
      "12 eeu RES\n",
      "ews oy\n",
      "\n",
      "oe Ke\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "riliceallenita\n",
      "\n",
      "‘an Pa\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "   \n",
      " \n",
      "\n",
      "[\n",
      "\n",
      "en\n",
      "\f\n",
      " \n",
      "\f\n",
      "LO KFS FS.\n",
      "Air-Cooled\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "mY .\n",
      "\n",
      "adidas\n",
      "\n",
      "aa ae\n",
      "\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "5 Shoebox LLC\n",
      "\n",
      "@gmail.com\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "     \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      "oe\n",
      "\n",
      "     \n",
      "\n",
      "RYU\n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n",
      " \n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "\n",
    "print(type(images[0]))\n",
    "def ocr_core(filename):\n",
    "    \"\"\"\n",
    "    This function will handle the core OCR processing of images.\n",
    "    \"\"\"\n",
    "    text = pytesseract.image_to_string(filename)  # We'll use Pillow's Image class to open the image and pytesseract to detect the string in the image\n",
    "    return text\n",
    "l=len(images)\n",
    "for i in range(l):\n",
    "    try:\n",
    "        temp=ocr_core(images[i])\n",
    "        if(temp):\n",
    "            print(temp)\n",
    "    except:\n",
    "        print('image not recognized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import sys\n",
    "#print(sys.version)\n",
    "#%pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(1, (3, 3), activation='relu', input_shape=(max_width, max_height, 3)))\n",
    "model.add(layers.MaxPooling2D((4, 4)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dropout(rate=.8))\n",
    "#model.add(layers.Dense(4, activation='relu'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "brand_dict = {}\n",
    "num = 0\n",
    "labels = []\n",
    "for b in Colors:\n",
    "    if b not in brand_dict:\n",
    "        brand_dict[b] = num\n",
    "        num+=1\n",
    "    labels.append(brand_dict[b])\n",
    "\n",
    "m = 400\n",
    "n = round(m*.8)\n",
    "train_images = np.asarray(image_array[0:n])\n",
    "test_images = np.asarray(image_array[n:m])\n",
    "train_labels = np.asarray(labels[0:n])\n",
    "test_labels = np.asarray(labels[n:m])\n",
    "print(type(test_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 130s 4s/step - loss: 13.6627 - accuracy: 0.5094 - val_loss: 13.1114 - val_accuracy: 0.3375\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=1, batch_size = 10,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}