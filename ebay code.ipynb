{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "akshit_df = './mlchallenge_set_2021.tsv'\n",
    "akshit_valid = './mlchallenge_set_validation.tsv'\n",
    "sam_df = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_2021_edited.txt'\n",
    "sam_valid = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_validation.tsv'\n",
    "#SA_valid=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_validation.tsv',header=None)\n",
    "#SA_df=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_2021.tsv',header=None)\n",
    "#df=SA_df\n",
    "#valid=SA_valid\n",
    "df = pd.read_table(sam_df)\n",
    "valid = pd.read_table(sam_valid,sep='\\t')\n",
    "df.columns=['category','primary_image_url','All Links','Tags','index']\n",
    "valid.columns=['ID', 'Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import test output file as dataframe and set the index.\n",
    "\"\"\"\n",
    "output = './akshit.tsv'\n",
    "predictions = pd.read_table(output,header=None)\n",
    "predictions.columns = ['ID', 'Group']\n",
    "predictions.set_index('ID', inplace=True)\n",
    "valid.set_index('ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute confusion matrix for validation data. Use valid dataframe to extract values from a tsv output file.\n",
    "\"\"\"\n",
    "preds = []\n",
    "actuals = []\n",
    "for index, row in valid.iterrows():\n",
    "    preds.append(predictions.loc[index]['Group'])\n",
    "    actuals.append(row['Group'])\n",
    "# print(preds[100:200])\n",
    "# print(actuals[100:200])\n",
    "def evaluate(actuals, preds):\n",
    "    C = pair_confusion_matrix(actuals, preds)\n",
    "    precision = C[1][1] / (C[1][1] + C[0][1]) # (true positives) / (true positives + false positives)\n",
    "    recall = C[1][1] / (C[1][1] + C[1][0]) # (true positives) / (true positives + false negatives)\n",
    "    print(f'Confusion Matrix:\\n{C}')\n",
    "    print(f'precision: {precision}')\n",
    "    print(f'recall: {recall}')\n",
    "    print(f'f1 score: {2 * precision * recall / (precision + recall)}')\n",
    "\n",
    "evaluate(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split links into list (for now only does first 100)\n",
    "i = 0\n",
    "for link in df['All Links'][0:100]:\n",
    "    df['All Links'][i] = link.split(';')\n",
    "    i+=1\n",
    "print(df['All Links'][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Frequency counting of the tags in a particular category.\n",
    "\"\"\"\n",
    "import re\n",
    "from collections import Counter\n",
    "# for category in range(1, 6):\n",
    "df = SA_df\n",
    "df = df.loc[df['category'] == 5]\n",
    "freq=Counter()\n",
    "attribute=[['']]*len(df)\n",
    "print('allocated array')\n",
    "trialrange=len(df)\n",
    "for x in range(0, trialrange, 5):#range(int(len(df)/10)):#len(df)\n",
    "    attribute[x]=df.iloc[x,3].lower()\n",
    "    attribute[x] = attribute[x][1:-1] # remove starting and ending parentheses\n",
    "    attribute[x] = re.split(r',(?![^(]*\\))', attribute[x]) # ignore commas that are inside parentheses\n",
    "    attribute[x] = [a.split(':') for a in attribute[x]]\n",
    "    freq+=Counter([i[0] for i in attribute[x]])\n",
    "    tempdict={}\n",
    "#     print(df.iloc[x,3])\n",
    "    #print(attribute[x])\n",
    "#     print(attribute)\n",
    "    for i in attribute[x]:\n",
    "        try:\n",
    "            tempdict[i[0]]=float(i[1])\n",
    "        except:\n",
    "            try:\n",
    "                tempdict[i[0]]=i[1]\n",
    "            except:\n",
    "                pass\n",
    "    attribute[x]=tempdict\n",
    "#     print(attribute[x])\n",
    "df = SA_df\n",
    "print(\"am out of loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict((k, v) for k, v in freq.items() if v > 20)\n",
    "array = list(sorted(d, key=lambda k: d[k],reverse=True))[:10]\n",
    "d= {k: v for k, v in sorted(d.items(), key=lambda item: item[1],reverse=True)}\n",
    "print(d)\n",
    "print(array)\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.pyplot as plt\n",
    "plt.bar(d.keys(),d.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = SA_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "validation_rows = df.loc[df['index'].isin(valid.index)]\n",
    "validation_rows = validation_rows.loc[validation_rows['category'] == 1]\n",
    "keep_tags = ['brand', 'size type', \"bottoms size (women's)\", 'material', 'inseam', 'color', 'rise', 'style', 'silhouette', 'country/region of manufacture']\n",
    "# df = category5\n",
    "def register_attributes(attribute, all_attributes):\n",
    "    attribute = attribute[1:-1].lower() # attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',(?![^(]*\\))', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        if attribute[i][0] in keep_tags:\n",
    "            all_attributes.add(attribute[i][0])\n",
    "    #print(f'atttribute is: {attribute}')\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return(attribute)\n",
    "\n",
    "def map_attributes(attribute, num_attributes, index_to_attr):\n",
    "    attribute = attribute[1:-1].lower() # attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',(?![^(]*\\))', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    all_attributes_for_row = [None] * num_attributes\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        #print(f'index: {attr_to_index[attribute[i][0]]}')\n",
    "        if len(attribute[i]) > 1 and attribute[i][0] in keep_tags:\n",
    "            all_attributes_for_row[attr_to_index[attribute[i][0]]] = attribute[i][1]\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return all_attributes_for_row\n",
    "m = 2000\n",
    "all_attributes = set()\n",
    "all_maps = []\n",
    "for index,row in validation_rows[0:].iterrows():\n",
    "    register_attributes(row['Tags'], all_attributes)\n",
    "\n",
    "all_attributes = list(all_attributes)\n",
    "attr_to_index = {all_attributes[i]: i for i in range(len(all_attributes))}\n",
    "#print(attr_to_index)\n",
    "#print(f'numAttributes: {len(all_attributes)}')\n",
    "\n",
    "for index,row in validation_rows[0:].iterrows():\n",
    "    all_maps.append(map_attributes(row['Tags'], len(all_attributes), attr_to_index))\n",
    "possible_vals = set()\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "for v in all_maps:\n",
    "    print(v)\n",
    "    possible_vals.update(v)\n",
    "    c.update(v)\n",
    "print(len(possible_vals))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with attribute values\n",
    "categories = pd.DataFrame(all_maps)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x = OneHotEncoder().fit_transform(categories)\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=0,verbose=True, n_init=2).fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analysis of K-Means output\n",
    "\"\"\"\n",
    "count = {}\n",
    "preds = []\n",
    "actuals = []\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    if label not in count:\n",
    "        count[label] = 0\n",
    "    count[label] += 1\n",
    "#     print(f'row {i}\\n: {df.iloc[i]}')\n",
    "    ID = validation_rows.iloc[i]['index']\n",
    "    if ID in valid.index:\n",
    "        preds.append(label)\n",
    "        actual = valid.loc[ID]['Group']\n",
    "        actuals.append(actual)\n",
    "#     print(f'ID: {ID}, pred: {label}, actual: {actual}')\n",
    "print(count)\n",
    "print(kmeans.inertia_)\n",
    "\n",
    "evaluate(actuals, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def cluster(data):\n",
    "    clustering = AgglomerativeClustering(compute_distances=True,compute_full_tree = True,distance_threshold = 0.1,n_clusters=None).fit(data)\n",
    "    print(clustering)\n",
    "#     print(\"LABELS\")\n",
    "#     print(clustering.labels_)\n",
    "#     print(len(clustering.labels_))\n",
    "#     print(len(set(clustering.labels_)))\n",
    "#     print(\"DISTAnCES\")\n",
    "#     print(clustering.distances_)\n",
    "#     print(\"num connected components\")\n",
    "#     print(clustering.n_connected_components_)\n",
    "    # make this better\n",
    "    # make this work on the entire dataset\n",
    "    # fix nonetypes\n",
    "    # don't punish missing attributes, but punish conflicts. how do we encode this?\n",
    "\n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = x.toarray() # only run this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x *= 10\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_clustering = [0] * len(x)\n",
    "count_so_far = 0\n",
    "for label in count:\n",
    "    print(f'now clustering group {label}')\n",
    "    indices = [i for i in range(x.shape[0]) if kmeans.labels_[i] == label]\n",
    "    data = [x[i] for i in indices]\n",
    "    c = cluster(data)\n",
    "    for i, label in enumerate(c.labels_):\n",
    "        full_clustering[indices[i]] = label + count_so_far\n",
    "#     print(c.labels_)\n",
    "    count_so_far += len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for i, label in enumerate(full_clustering):\n",
    "    if label not in groups:\n",
    "        groups[label] = []\n",
    "    groups[label].append(i)\n",
    "groups = {label: groups[label] for label in groups if len(groups[label]) > 1}\n",
    "print(groups)\n",
    "for label in groups:\n",
    "    print(f'GROUP: {label}')\n",
    "    for item in groups[label]:\n",
    "        print(validation_rows['Tags'].iloc[[item]].to_string())\n",
    "    print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate clustering after it has gone through k-means and agglomorative clustering.\n",
    "\"\"\"\n",
    "p = full_clustering\n",
    "print(full_clustering)\n",
    "actuals = []\n",
    "for i in range(len(p)):\n",
    "    ID = validation_rows.iloc[i]['index']\n",
    "    if ID in valid.index:\n",
    "        actual = valid.loc[ID]['Group']\n",
    "        actuals.append(actual)\n",
    "a = actuals # valid['Group'].tolist()\n",
    "print(len(set(p)))\n",
    "print(len(p))\n",
    "evaluate(a, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions used to display the tags in a nicer manner below.\n",
    "\"\"\"\n",
    "def get_mapping(attribute):\n",
    "    attribute = re.sub(r'[()]','', attribute)\n",
    "#     attribute = re.split(r',', attribute)\n",
    "#     attribute = [a.split(':') for a in attribute]\n",
    "    attribute = re.split(r':+', attribute)\n",
    "    attribute = [a.split(',') for a in attribute]\n",
    "    '''\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        all_attributes.add(attribute[i][0])\n",
    "    #print(f'atttribute is: {attribute}')\n",
    "    '''\n",
    "    mapping = {}\n",
    "    for i in range(len(attribute) - 1):\n",
    "        if i == len(attribute) - 2:\n",
    "            mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "        else:\n",
    "            mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return mapping\n",
    "\n",
    "def mapping_to_string(mapping):\n",
    "    keys = list(mapping.keys())\n",
    "    keys.sort()\n",
    "    return ';'.join([f'{key}:{list(sorted(mapping[key]))}' for key in keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Show the items where the two clusterings ,p(redict) and a(ctual), differ.\n",
    "\"\"\"\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(p)):\n",
    "        if i != j:\n",
    "            if a[i] == a[j] and p[i] != p[j]:\n",
    "                print(\"These two items should be in the same group:\")\n",
    "            elif a[i] != a[j] and p[i] == p[j]:\n",
    "                continue\n",
    "                print(\"These two items should not be in the same group\")\n",
    "            else:\n",
    "                continue\n",
    "            print(f\"tags: {mapping_to_string(get_mapping(validation_rows['Tags'].iloc[[i]].to_string(index=False)))}\")\n",
    "            print(f\"urls: {validation_rows['primary_image_url'].iloc[[i]].to_string()}\")\n",
    "            print(f\"tags: {mapping_to_string(get_mapping(validation_rows['Tags'].iloc[[j]].to_string(index=False)))}\")\n",
    "            print(f\"urls: {validation_rows['primary_image_url'].iloc[[j]].to_string()}\")\n",
    "            print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_clustering = full_clustering\n",
    "print(old_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    #response = requests.get(url)\n",
    "    #img = Image.open(BytesIO(response.content))\n",
    "    link_labels = [df['Tags'][i] for i in clustering.labels_]\n",
    "    dendrogram(linkage_matrix, labels = link_labels)\n",
    "plot_dendrogram(clustering)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "freq=Counter()\n",
    "attribute=[['']]*len(df)\n",
    "trialrange=5000\n",
    "for x in range(trialrange):#range(int(len(df)/10)):#len(df)\n",
    "    attribute[x]=df.iloc[x,3].lower()\n",
    "    attribute[x] = re.sub(r'[()]','', attribute[x])\n",
    "    attribute[x] = re.split(r',', attribute[x])\n",
    "    attribute[x] = [a.split(':') for a in attribute[x]]\n",
    "    freq+=Counter([i[0] for i in attribute[x]])\n",
    "    tempdict={}\n",
    "    for i in attribute[x]:\n",
    "\n",
    "            try:\n",
    "                tempdict[i[0]]=float(i[1])\n",
    "            except:\n",
    "                try:\n",
    "                    tempdict[i[0]]=i[1]\n",
    "                except:\n",
    "                    pass\n",
    "    attribute[x]=tempdict\n",
    "\n",
    "df['seg']=attribute\n",
    "#print(df['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "Images=[]\n",
    "Colors = []\n",
    "color_images = []\n",
    "print(trialrange)\n",
    "for i in range(trialrange):\n",
    "    try:\n",
    "        #df['seg'].iloc[i]['brand']\n",
    "        #df['primary_image_url'].iloc[i]\n",
    "        #print(df['seg'].iloc[i]['brand'])\n",
    "        #print(df['primary_image_url'].iloc[i])\n",
    "        if df['seg'].iloc[i]['brand'] == 'nike' or df['seg'].iloc[i]['brand'] == 'adidas':\n",
    "            Brands.append(df['seg'].iloc[i]['brand'])\n",
    "            Images.append(df['primary_image_url'].iloc[i])\n",
    "        if df['seg'].iloc[i]['color'] == 'black' or df['seg'].iloc[i]['color'] == 'white':\n",
    "            Colors.append(df['seg'].iloc[i]['color'])\n",
    "            color_images.append(df['primary_image_url'].iloc[i])\n",
    "    except:\n",
    "        continue\n",
    "        #Brands.remove[-1]\n",
    "        #print('not possible at: ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brands))\n",
    "print(len(Colors))\n",
    "print(len(color_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "'''\n",
    "url = df['primary_image_url'][4]\n",
    "response = requests.get(url)\n",
    "#img = Image.open(BytesIO(response.content))\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "img.show()\n",
    "result = Image.new(img.mode, (1000, 550), (64,64,64))\n",
    "result.paste(img, (0, 0))\n",
    "result.show()\n",
    "print(np.asarray(img).shape)\n",
    "print(np.asarray(result).shape)\n",
    "'''\n",
    "image_array = []\n",
    "images = []\n",
    "max_height = 0\n",
    "max_width = 0\n",
    "i = 0\n",
    "for img in color_images[0:n]:\n",
    "    response = requests.get(img)\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    if np.asarray(img).shape[1] > max_width:\n",
    "        max_width = np.asarray(img).shape[1]\n",
    "    if np.asarray(img).shape[0] > max_height:\n",
    "        max_height = np.asarray(img).shape[0]\n",
    "    images.append(img)\n",
    "i=0\n",
    "for img in images:\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    try:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode, (max_width, max_height), (64,64,64))\n",
    "    except:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode,(max_width,max_height),(64,64,64))\n",
    "    margin.paste(img, (0, 0))\n",
    "    image_array.append(np.asarray(margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import sys\n",
    "#print(sys.version)\n",
    "#%pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(1, (3, 3), activation='relu', input_shape=(max_width, max_height, 3)))\n",
    "model.add(layers.MaxPooling2D((4, 4)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dropout(rate=.8))\n",
    "#model.add(layers.Dense(4, activation='relu'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_dict = {}\n",
    "num = 0\n",
    "labels = []\n",
    "for b in Colors:\n",
    "    if b not in brand_dict:\n",
    "        brand_dict[b] = num\n",
    "        num+=1\n",
    "    labels.append(brand_dict[b])\n",
    "\n",
    "m = 400\n",
    "n = round(m*.8)\n",
    "train_images = np.asarray(image_array[0:n])\n",
    "test_images = np.asarray(image_array[n:m])\n",
    "train_labels = np.asarray(labels[0:n])\n",
    "test_labels = np.asarray(labels[n:m])\n",
    "print(type(test_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=1, batch_size = 10,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.9.1), accessed by source activate my",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
