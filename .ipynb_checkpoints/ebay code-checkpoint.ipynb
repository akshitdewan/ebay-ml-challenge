{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "akshit_df = './mlchallenge_set_2021.tsv'\n",
    "akshit_valid = './mlchallenge_set_validation.tsv'\n",
    "sam_df = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_2021_edited.txt'\n",
    "sam_valid = 'C:/Users/sjmal/OneDrive/Desktop/ML/2021/mlchallenge_set_validation.tsv'\n",
    "#SA_valid=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_validation.tsv',header=None)\n",
    "#SA_df=pd.read_table('/Users/shivankagrawal/Documents/ebay/mlchallenge_set_2021.tsv',header=None)\n",
    "#df=SA_df\n",
    "#valid=SA_valid\n",
    "df = pd.read_table(sam_df)\n",
    "valid = pd.read_table(sam_valid,sep='\\t')\n",
    "df.columns=['category','primary_image_url','All Links','Tags','index']\n",
    "valid.columns=['ID', 'Group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of groups with list of IDs in that group\n",
    "dict = {}\n",
    "total=0\n",
    "for index,row in valid.iterrows():\n",
    "    if row['Group'] in dict:\n",
    "        dict[row['Group']].append(row['ID'])\n",
    "    else:\n",
    "        dict[row['Group']] = [row['ID']]\n",
    "#list of groups with multiple IDs\n",
    "matches = []\n",
    "for key,value in dict.items():\n",
    "    if len(value) > 1:\n",
    "        total+=len(value)\n",
    "        matches.append(key)\n",
    "print(len(matches))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())\n",
    "print(valid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split links into list (for now only does first 100)\n",
    "i = 0\n",
    "for link in df['All Links'][0:100]:\n",
    "    df['All Links'][i] = link.split(';')\n",
    "    i+=1\n",
    "print(df['All Links'][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def register_attributes(attribute, all_attributes):\n",
    "    attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        all_attributes.add(attribute[i][0])\n",
    "    #print(f'atttribute is: {attribute}')\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return(attribute)\n",
    "\n",
    "def map_attributes(attribute, num_attributes, index_to_attr):\n",
    "    attribute = re.sub(r'[()]','', attribute)\n",
    "    attribute = re.split(r',', attribute)\n",
    "    attribute = [a.split(':') for a in attribute]\n",
    "    all_attributes_for_row = [None] * num_attributes\n",
    "    for i, a in enumerate(attribute):\n",
    "        attribute[i] = [s.strip() for s in a]\n",
    "        #print(f'index: {attr_to_index[attribute[i][0]]}')\n",
    "        if len(attribute[i]) > 1:\n",
    "            all_attributes_for_row[attr_to_index[attribute[i][0]]] = attribute[i][1]\n",
    "    mapping = {}\n",
    "    #for i in range(len(attribute) - 1):\n",
    "    #    if i == len(attribute) - 2:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:]\n",
    "    #    else:\n",
    "    #        mapping[attribute[i][-1]] = attribute[i + 1][:-1]\n",
    "    return all_attributes_for_row\n",
    "m = 2000\n",
    "all_attributes = set()\n",
    "all_maps = []\n",
    "for index,row in df[0:m].iterrows():\n",
    "    register_attributes(row['Tags'], all_attributes)\n",
    "\n",
    "all_attributes = list(all_attributes)\n",
    "attr_to_index = {all_attributes[i]: i for i in range(len(all_attributes))}\n",
    "#print(attr_to_index)\n",
    "#print(f'numAttributes: {len(all_attributes)}')\n",
    "\n",
    "for index,row in df[0:m].iterrows():\n",
    "    all_maps.append(map_attributes(row['Tags'], len(all_attributes), attr_to_index))\n",
    "#print(all_maps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe with attribute values\n",
    "categories = pd.DataFrame(all_maps)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x = OneHotEncoder().fit_transform(categories)\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(compute_distances=True,compute_full_tree = True,distance_threshold = 1.5,n_clusters=None).fit(x.todense())\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LABELS\")\n",
    "print(clustering.labels_)\n",
    "print(len(clustering.labels_))\n",
    "print(len(set(clustering.labels_)))\n",
    "print(\"DISTAnCES\")\n",
    "print(clustering.distances_)\n",
    "print(\"num connected components\")\n",
    "print(clustering.n_connected_components_)\n",
    "# make this better\n",
    "# make this work on the entire dataset\n",
    "# fix nonetypes\n",
    "# don't punish missing attributes, but punish conflicts. how do we encode this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for i, label in enumerate(clustering.labels_):\n",
    "    if label not in groups:\n",
    "        groups[label] = []\n",
    "    groups[label].append(i)\n",
    "groups = {label: groups[label] for label in groups if len(groups[label]) > 1}\n",
    "print(groups)\n",
    "for label in groups:\n",
    "    print(f'GROUP: {label}')\n",
    "    for item in groups[label]:\n",
    "        print(df['Tags'][item])\n",
    "    print('-----------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    #response = requests.get(url)\n",
    "    #img = Image.open(BytesIO(response.content))\n",
    "    link_labels = [df['Tags'][i] for i in clustering.labels_]\n",
    "    dendrogram(linkage_matrix, labels = link_labels)\n",
    "plot_dendrogram(clustering)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "freq=Counter()\n",
    "attribute=[['']]*len(df)\n",
    "trialrange=5000\n",
    "for x in range(trialrange):#range(int(len(df)/10)):#len(df)\n",
    "    attribute[x]=df.iloc[x,3].lower()\n",
    "    attribute[x] = re.sub(r'[()]','', attribute[x])\n",
    "    attribute[x] = re.split(r',', attribute[x])\n",
    "    attribute[x] = [a.split(':') for a in attribute[x]]\n",
    "    freq+=Counter([i[0] for i in attribute[x]])\n",
    "    tempdict={}\n",
    "    for i in attribute[x]:\n",
    "\n",
    "            try:\n",
    "                tempdict[i[0]]=float(i[1])\n",
    "            except:\n",
    "                try:\n",
    "                    tempdict[i[0]]=i[1]\n",
    "                except:\n",
    "                    pass\n",
    "    attribute[x]=tempdict\n",
    "\n",
    "df['seg']=attribute\n",
    "#print(df['seg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "Images=[]\n",
    "Colors = []\n",
    "color_images = []\n",
    "print(trialrange)\n",
    "for i in range(trialrange):\n",
    "    try:\n",
    "        #df['seg'].iloc[i]['brand']\n",
    "        #df['primary_image_url'].iloc[i]\n",
    "        #print(df['seg'].iloc[i]['brand'])\n",
    "        #print(df['primary_image_url'].iloc[i])\n",
    "        if df['seg'].iloc[i]['brand'] == 'nike' or df['seg'].iloc[i]['brand'] == 'adidas':\n",
    "            Brands.append(df['seg'].iloc[i]['brand'])\n",
    "            Images.append(df['primary_image_url'].iloc[i])\n",
    "        if df['seg'].iloc[i]['color'] == 'black' or df['seg'].iloc[i]['color'] == 'white':\n",
    "            Colors.append(df['seg'].iloc[i]['color'])\n",
    "            color_images.append(df['primary_image_url'].iloc[i])\n",
    "    except:\n",
    "        continue\n",
    "        #Brands.remove[-1]\n",
    "        #print('not possible at: ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brands))\n",
    "print(len(Colors))\n",
    "print(len(color_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 400\n",
    "from PIL import Image, ImageFile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "'''\n",
    "url = df['primary_image_url'][4]\n",
    "response = requests.get(url)\n",
    "#img = Image.open(BytesIO(response.content))\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "img.show()\n",
    "result = Image.new(img.mode, (1000, 550), (64,64,64))\n",
    "result.paste(img, (0, 0))\n",
    "result.show()\n",
    "print(np.asarray(img).shape)\n",
    "print(np.asarray(result).shape)\n",
    "'''\n",
    "image_array = []\n",
    "images = []\n",
    "max_height = 0\n",
    "max_width = 0\n",
    "i = 0\n",
    "for img in color_images[0:n]:\n",
    "    response = requests.get(img)\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    if np.asarray(img).shape[1] > max_width:\n",
    "        max_width = np.asarray(img).shape[1]\n",
    "    if np.asarray(img).shape[0] > max_height:\n",
    "        max_height = np.asarray(img).shape[0]\n",
    "    images.append(img)\n",
    "i=0\n",
    "for img in images:\n",
    "    if i%200 == 0:\n",
    "        print(i)\n",
    "    i+=1\n",
    "    try:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode, (max_width, max_height), (64,64,64))\n",
    "    except:\n",
    "        img = img.convert('RGB')\n",
    "        margin = Image.new(img.mode,(max_width,max_height),(64,64,64))\n",
    "    margin.paste(img, (0, 0))\n",
    "    image_array.append(np.asarray(margin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import sys\n",
    "#print(sys.version)\n",
    "#%pip install tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(1, (3, 3), activation='relu', input_shape=(max_width, max_height, 3)))\n",
    "model.add(layers.MaxPooling2D((4, 4)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dropout(rate=.8))\n",
    "#model.add(layers.Dense(4, activation='relu'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_dict = {}\n",
    "num = 0\n",
    "labels = []\n",
    "for b in Colors:\n",
    "    if b not in brand_dict:\n",
    "        brand_dict[b] = num\n",
    "        num+=1\n",
    "    labels.append(brand_dict[b])\n",
    "\n",
    "m = 400\n",
    "n = round(m*.8)\n",
    "train_images = np.asarray(image_array[0:n])\n",
    "test_images = np.asarray(image_array[n:m])\n",
    "train_labels = np.asarray(labels[0:n])\n",
    "test_labels = np.asarray(labels[n:m])\n",
    "print(type(test_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=1, batch_size = 10,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "sjmalnati"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
